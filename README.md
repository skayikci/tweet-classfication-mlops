# ğŸ¦ Tweet Classification MLOps Pipeline

A production-grade MLOps pipeline for tweet sentiment classification â€” built with FastAPI, MLflow, Prefect, Evidently, and PostgreSQL.

This project showcases the full ML lifecycle: from model training and experiment tracking to API deployment and automated monitoring.

---

## ğŸš€ Features

- **Tweet Classification**: Predict tweet sentiment (positive/negative) using a scikit-learn pipeline.
- **MLflow Integration**: Track experiments, register models, and serve them via FastAPI.
- **Model Registry & Loading**: Dynamically load latest model version via MLflow.
- **Monitoring Pipeline**: Generate and visualize Evidently reports (data drift, classification quality).
- **Automated Testing**: Lightweight integration tests with pytest.
- **Modular Design**: Clear separation of `src/` code and `tests/`, supporting scalability.

---

## ğŸ› ï¸ Stack

| Layer            | Tool(s)                                   |
|------------------|-------------------------------------------|
| ğŸ§  Model         | Scikit-learn, TF-IDF, LabelEncoder        |
| ğŸ“Š Monitoring    | Evidently                                 |
| âš™ï¸ Orchestration | Prefect 2                                 |
| ğŸ“¦ Tracking      | MLflow                                    |
| ğŸŒ API           | FastAPI + Uvicorn                         |
| ğŸ›¢ï¸ Storage       | PostgreSQL (via Docker Compose)           |
| ğŸ§ª Testing       | Pytest + Faker                            |

---

## ğŸ³ Quickstart (via Docker Compose)

1. Clone the repo:
    ```bash
    git clone https://github.com/skayikci/tweet-classfication-mlops.git
    cd tweet-classfication-mlops
    ```

2. Start everything:

    1. Make sure Docker is running.
    2. Using make command:
    ```bash
    make run
    ```
    Or directly with Docker Compose:
    ```bash
    docker-compose up --build
    ```

3. Access services:
    - API: `http://localhost:8000/docs`
    - MLflow UI: `http://localhost:5555`
    - Prefect Orion: `http://localhost:4200`
    - PostgreSQL: `localhost:5432` (see `.env` for credentials)

---

## âš™ï¸ Project Structure

```bash
.
â”œâ”€â”€ monitoring/                    # Evidently monitoring flow
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tweet_classification.py    # Training & inference logic
â”‚   â”œâ”€â”€ api.py                     # FastAPI app with prediction endpoints
â”œâ”€â”€ tests/                         # Pytest-based integration & unit tests
â”œâ”€â”€ data/                          # Sample datasets and reports
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

---
## ğŸ‘ï¸ Setup instructions (Local dev)
```bash
# Create a virtual environment
python3 -m venv venv
source venv/bin/activate
# Install dependencies
pip install -r requirements.txt
```

---

## ğŸš† Model Training & Inference
1. Make sure Docker is running.
2. Using make command:
   ```bash
   make orchestrate
   ```
   Or directly with Python:
   ```bash
   python src/tweet_classification.py
   ```
This will train the model and save it to MLflow as seen in the MLflow UI.
<p align="center">
  <img src="assets/preview.webp" alt="Model Training Preview" width="600"/>
</p>

## ğŸ† Selecting the best model
1. Make sure Docker is running.
2. The orchestration flow will automatically select the best model based on the evaluation metrics. And tag it as `production` with an alias.

<p align="center">
  <img src="assets/svm_selection.png" alt="SVM Model Selection" width="600"/>
</p>

3. Then this model will be served via FastAPI, ready for predictions as seen in code:
```python
try:
    logger.info(f"ğŸ“¡ Connecting to MLflow at {MLFLOW_TRACKING_URI}")
    model_uri = f"models:/{MODEL_NAME}@production"
    logger.info(f"ğŸ“¦ Loading model from MLflow URI: {model_uri}")
    model = mlflow.pyfunc.load_model(model_uri)
    logger.info("âœ… Model loaded from MLflow.")
except Exception as e:
    logger.exception(f"âŒ Failed to load model from MLflow. {e}")
    raise
```

## ğŸ§ª Run Tests

1. Make sure Docker is running.
2. Using make command:
  1. Using make command:
  ```bash
  make test
  ```
  2. Or directly with pytest:
  ```bash
  PYTHONPATH=. pytest tests -v
  ```

---

## ğŸ“Š Generate Monitoring Reports

1. Make sure Docker is running.
2. Generate reports
  1. Using make command:
  ```bash
  make monitoring
  ```
  2. Or directly with Prefect:
  ```bash
  python monitoring/monitoring_flow.py
  ```

This saves an interactive HTML report to `data/monitoring/`.

---

## ğŸ“¥ Sample Prediction

1. Go to the web page for prediction and user input:
```bash
http://localhost:8000/
```
You can enter a tweet text and get the sentiment prediction.
<p align="center">
  <img src="assets/user_label_feedbacks.png" alt="User Label Feedbacks" width="600"/>
</p>

This website also has latest drift report of the model, which is generated by Evidently.

2. Alternatively, use the API directly:

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "This product is amazing!"}'
```

Or simply use the Swagger UI at `http://localhost:8000/docs`.


---

## ğŸ“Œ Use Cases

- Demonstrating **end-to-end MLOps workflows**
- Teaching **monitoring and reproducibility**
- Testing ML APIs in **Dockerized setups**
- Preparing for **production-grade ML deployment**

---

## â„¹ï¸ Additional Information
- For further details, checkout assets in the `assets/` directory for category distribution, confusion matrix, and more.

---

## ğŸ§  Author

Built with love by [Serhat KayÄ±kÃ§Ä±](https://serhat.tech)
---
